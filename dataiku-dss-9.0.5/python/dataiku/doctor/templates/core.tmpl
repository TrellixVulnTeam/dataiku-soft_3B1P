### {{ title }}

##### Notebook automatically generated from your model

## Model {{ model_name }}, trained on {{ model_date.isoformat(' ') }}.

###### Generated on {{ now }}

{% block head %}
{% endblock %}

###### Warning

## The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps
## of training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not
## replicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine
## learning model.

{% block imports %}
## Let's start with importing the required libs :
import sys
import dataiku
import numpy as np
import pandas as pd
import sklearn as sk
import dataiku.core.pandasutils as pdu
from dataiku.doctor.preprocessing import PCA
from collections import defaultdict, Counter
{% endblock %}

## And tune pandas display options:
pd.set_option('display.width', 3000)
pd.set_option('display.max_rows', 200)
pd.set_option('display.max_columns', 200)


###### Importing base data

## The first step is to get our machine learning dataset:

# We apply the preparation that you defined. You should not modify this.
preparation_steps = {{script_steps}}
preparation_output_schema = {{preparation_output_schema}}

ml_dataset_handle = dataiku.Dataset('{{ dataset }}')
ml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)
%time ml_dataset = ml_dataset_handle.get_dataframe(limit = {{split_stuff.splitSamplingHeadRows }})

print ('Base data has %i rows and %i columns' % (ml_dataset.shape[0], ml_dataset.shape[1]))
# Five first records",
ml_dataset.head(5)

###### Initial data management

## The preprocessing aims at making the dataset compatible with modeling.
## At the end of this step, we will have a matrix of float numbers, with no missing values.
## We'll use the features and the preprocessing steps defined in Models.
##
## Let's only keep selected features

ml_dataset = ml_dataset[{{input_columns}}]

## Let's first coerce categorical columns into unicode, numerical features into floats.

# astype('unicode') does not work as expected

def coerce_to_unicode(x):
    if sys.version_info < (3, 0):
        if isinstance(x, str):
            return unicode(x,'utf-8')
        else:
            return unicode(x)
    else:
        return str(x)


categorical_features = {{ categorical_features }}
numerical_features = {{ numerical_features }}
text_features = {{ text_features }}
from dataiku.doctor.utils import datetime_to_epoch
for feature in categorical_features:
    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)
for feature in text_features:
    ml_dataset[feature] = ml_dataset[feature].apply(coerce_to_unicode)
for feature in numerical_features:
    if ml_dataset[feature].dtype == np.dtype('M8[ns]') or (hasattr(ml_dataset[feature].dtype, 'base') and ml_dataset[feature].dtype.base == np.dtype('M8[ns]')):
        ml_dataset[feature] = datetime_to_epoch(ml_dataset[feature])
    else:
        ml_dataset[feature] = ml_dataset[feature].astype('double')

{% block profiling_copy %}
{% endblock %}


{% block prebody %}
{% endblock %}


{% block preprocessing %}
###### Features preprocessing

## The first thing to do at the features level is to handle the missing values.
## Let's reuse the settings defined in the model

drop_rows_when_missing = {{ handle_missing.drop_rows_when_missing }}
impute_when_missing = {{ handle_missing.impute_when_missing }}

# Features for which we drop rows with missing values"
for feature in drop_rows_when_missing:
    train = train[train[feature].notnull()]
    {% if is_supervized %}test = test[test[feature].notnull()]{% endif %}
    print ('Dropped missing records in %s' % feature)

# Features for which we impute missing values"
for feature in impute_when_missing:
    if feature['impute_with'] == 'MEAN':
        v = train[feature['feature']].mean()
    elif feature['impute_with'] == 'MEDIAN':
        v = train[feature['feature']].median()
    elif feature['impute_with'] == 'CREATE_CATEGORY':
        v = 'NULL_CATEGORY'
    elif feature['impute_with'] == 'MODE':
        v = train[feature['feature']].value_counts().index[0]
    elif feature['impute_with'] == 'CONSTANT':
        v = feature['value']
    train[feature['feature']] = train[feature['feature']].fillna(v)
    {% if is_supervized %}test[feature['feature']] = test[feature['feature']].fillna(v){% endif %}
    print ('Imputed missing values in feature %s with value %s' % (feature['feature'], coerce_to_unicode(v)))


## We can now handle the categorical features (still using the settings defined in Models):


{% if categorical_processing.DUMMIFY %}

## Let's dummy-encode the following features.
## A binary column is created for each of the 100 most frequent values.

LIMIT_DUMMIES = 100

categorical_to_dummy_encode = {{ categorical_processing.DUMMIFY }}

# Only keep the top 100 values
def select_dummy_values(train, features):
    dummy_values = {}
    for feature in categorical_to_dummy_encode:
        values = [
            value
            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)
        ]
        dummy_values[feature] = values
    return dummy_values

DUMMY_VALUES = select_dummy_values(train, categorical_to_dummy_encode)

def dummy_encode_dataframe(df):
    for (feature, dummy_values) in DUMMY_VALUES.items():
        for dummy_value in dummy_values:
            dummy_name = u'%s_value_%s' % (feature, coerce_to_unicode(dummy_value))
            df[dummy_name] = (df[feature] == dummy_value).astype(float)
        del df[feature]
        print ('Dummy-encoded feature %s' % feature)

dummy_encode_dataframe(train)
{% if is_supervized %}
dummy_encode_dataframe(test)
{% endif %}

{% endif %}


{% if categorical_processing.IMPACT %}

## Let's impact code the following featuress.

categorical_to_impact_code = {{ categorical_processing.IMPACT }}

# impact coding
from dataiku.notebook import ImpactCoding

impact_coding = ImpactCoding('{{ categorical_processing.impact_method }}', m=10)  # tune m
train = pd.concat([train, impact_coding.fit_transform(train[categorical_to_impact_code], train['__target__'])], axis=1)
for feature in categorical_to_impact_code:
    del train[feature]
{% if is_supervized %}
test = pd.concat([test, impact_coding.transform(test[categorical_to_impact_code])], axis=1)
for feature in categorical_to_impact_code:
    del test[feature]
{% endif %}
for feature in categorical_to_impact_code:
    print ('Impact Coded feature %s ' % feature)

{% endif %}


{% if categorical_processing.FLAG_PRESENCE %}

## Let's replace these features by a simple 0/1 flag indicating presence

categorical_to_replace_by_presence = {{categorical_processing.FLAG_PRESENCE }}

# Features for which we replace the feature by a 'flag feature' indicating whether the value was present"
for feature in categorical_to_replace_by_presence:
    train['present_' + feature] = train[feature].map(lambda x: 0 if pd.isnull(x) else 1).astype(np.uint8)
    del train[feature]
    {% if is_supervized %}
    test['present_' + feature] = test[feature].map(lambda x: 0 if pd.isnull(x) else 1).astype(np.uint8)
    del test[feature]
    {% endif %}
    print ('Flagged missing/present values in feature %s' % feature)

{% endif %}


{% if rescale_features %}

## Let's rescale numerical features

rescale_features = {{ rescale_features }}
for (feature_name, rescale_method) in rescale_features.items():
    if rescale_method == 'MINMAX':
        _min = train[feature_name].min()
        _max = train[feature_name].max()
        scale = _max - _min
        shift = _min
    else:
        shift = train[feature_name].mean()
        scale = train[feature_name].std()
    if scale == 0.:
        del train[feature_name]
        {% if is_supervized %}del test[feature_name]{% endif %}
        print ('Feature %s was dropped because it has no variance' % feature_name)
    else:
        print ('Rescaled %s' % feature_name)
        train[feature_name] = (train[feature_name] - shift).astype(np.float64) / scale
        {% if is_supervized %}test[feature_name] = (test[feature_name] - shift).astype(np.float64) / scale{% endif %}

{% else %}
# Rescaling is not required
{% endif %}

{% if text_features %}

# Text Features

from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.decomposition  import TruncatedSVD

text_svds = {}
for text_feature in text_features:
    n_components = 50
    text_svds[text_feature] = TruncatedSVD(n_components=n_components)
    s = HashingVectorizer(n_features=100000).transform(train[text_feature])
    text_svds[text_feature].fit(s)
    train_transformed = text_svds[text_feature].transform(s)
    {% if is_supervized %}
    test_transformed = text_svds[text_feature].transform(HashingVectorizer(n_features=100000).transform(test[text_feature]))
    {% endif %}
    for i in range(0, n_components):
        train[text_feature + ":text:" + str(i)] = train_transformed[:,i]
        {% if is_supervized %}
        test[text_feature + ":text:" + str(i)] = test_transformed[:,i]
        {% endif %}
    train.drop(text_feature, axis=1, inplace=True)
    {% if is_supervized %}
    test.drop(text_feature, axis=1, inplace=True)
    {% endif %}
{% endif %}


{% endblock %}


{% block extract_target %}
{% endblock %}


{% block dimension_reduction %}

{% if reduce.enabled %}
###### Dimension Reduction

## Finally, the last step before training the dimensionality reduction using a Principal Components Analysis.


# Looking for the number of components to keep
pca = PCA({{ reduce.kept_variance }}, normalize=True,)
# Training the PCA
pca.fit(train)

# Apply it on train set

{% if is_supervized %}
# Stick the target back into the train dataset
train = pca.transform(train).join(target_train)
# Apply it on test set
test = pca.transform(test).join(target_test)
{% else%}
train = pca.transform(train)
{% endif%}
{% endif %}

{% endblock %}

###### Modeling


{% block modeling %}
{% endblock %}

## Build up our result dataset
{% block prediction %}
{% endblock %}


{% block feature_importance %}
{% if enable_feature_selection %}
## Let's have a look at feature importances
feature_importances_data = []
features = X_train.columns
for feature_name, feature_importance in zip(features, clf.feature_importances_):
    feature_importances_data.append({
        'feature': feature_name,
        'importance': feature_importance
    })

# Plot the results
pd.DataFrame(feature_importances_data)\
    .set_index('feature')\
    .sort_values(by='importance')[-10::]\
    .plot(title='Top 10 most important variables',
          kind='barh',
          figsize=(10, 6),
          color='#348ABD',
          alpha=0.6,
          lw='1',
          edgecolor='#348ABD',
          grid=False,)
{% endif %}
{% endblock %}

###### Results

{% block evaluation %}
{% endblock %}


## That's it. It's now up to you to tune your preprocessing, your algo, and your analysis !
##

