{% extends "prediction.tmpl" %}

{% block target_variable %}
## We renamed the target variable to a column named target
ml_dataset['__target__'] = ml_dataset['{{target}}']
del ml_dataset['{{target}}']
{% endblock %}




{% block model_definition %}

{% if algorithm == "RANDOM_FOREST_REGRESSION" %}
from sklearn.ensemble import RandomForestRegressor
clf = RandomForestRegressor(n_estimators={{ post_train.rf.estimators }},
    random_state=1337,
    max_depth={{ post_train.rf.max_tree_depth if post_train.rf.max_tree_depth > 0 else None }},
    min_samples_leaf={{ post_train.rf.min_samples_leaf }},
    verbose=2)

{% elif algorithm == "GBT_REGRESSION" %}
from sklearn.ensemble import GradientBoostingRegressor
clf = GradientBoostingRegressor(
                    random_state = 1337,
                    verbose = 0,
                    n_estimators = {{ post_train.gbt.n_estimators }},
                    learning_rate = {{ post_train.gbt.learning_rate }},
                    loss = '{{ post_train.gbt.loss }}',
                    max_depth = {{ post_train.gbt.max_depth }}
                   )

{% elif algorithm == "EXTRA_TREES" %}
from sklearn.ensemble import ExtraTreesRegressor
clf = ExtraTreesRegressor(
                    n_estimators={{ post_train.extra_trees.estimators }},
                    random_state=1337,
                    max_depth={{ post_train.extra_trees.max_tree_depth if post_train.extra_trees.max_tree_depth > 0 else None }},
                    min_samples_leaf={{ post_train.extra_trees.min_samples_leaf }},
                    verbose=2
                   )

{% elif algorithm == "XGBOOST_REGRESSION" %}
import xgboost as xgb
clf = xgb.XGBRegressor(
                    max_depth={{ post_train.xgboost.max_depth }},
                    learning_rate={{ post_train.xgboost.learning_rate }},
                    gamma={{ post_train.xgboost.gamma }},
                    min_child_weight={{ post_train.xgboost.min_child_weight }},
                    max_delta_step={{ post_train.xgboost.max_delta_step }},
                    subsample={{ post_train.xgboost.subsample }},
                    colsample_bytree={{ post_train.xgboost.colsample_bytree }},
                    colsample_bylevel={{ post_train.xgboost.colsample_bylevel }},
                    reg_alpha={{ post_train.xgboost.alpha }},
                    reg_lambda={{ post_train.xgboost.lambda }},
                    n_estimators={{ post_train.xgboost.n_estimators }},
                    silent=0,
                    nthread={{ post_train.xgboost.nthread }},
                    scale_pos_weight={{ post_train.xgboost.scale_pos_weight }},
                    base_score={{ post_train.xgboost.base_score }},
                    seed={{ post_train.xgboost.seed }},
                    missing={{post_train.xgboost.missing if post_train.xgboost.impute_missing else None}},
                  )

{% elif algorithm == "KNN" %}
from sklearn.neighbors import KNeighborsRegressor
clf = KNeighborsRegressor(n_neighbors={{post_train.knn.k}},
                          weights={{"'distance'" if pre_train.knn_grid.distance_weighting else "'uniform'"}},
                          algorithm='{{pre_train.knn_grid.algorithm}}',
                          leaf_size={{pre_train.knn_grid.leaf_size}},
                          p={{pre_train.knn_grid.p}})

{% elif algorithm == "NEURAL_NETWORK" %}
from sklearn.neural_network import MLPRegressor
clf = MLPRegressor(hidden_layer_sizes={{ pre_train.neural_network_grid.layer_sizes.get("values")[0] }},
                     activation='{{ pre_train.neural_network_grid.activation }}',
                     solver='{{ pre_train.neural_network_grid.solver }}',
                     alpha={{ pre_train.neural_network_grid.alpha }},
                     batch_size={{ "'auto'" if pre_train.neural_network_grid.auto_batch else pre_train.neural_network_grid.batch_size }},
                     max_iter={{ pre_train.neural_network_grid.max_iter }},
                     random_state={{ pre_train.neural_network_grid.seed }},
                     tol={{ pre_train.neural_network_grid.tol }},
                     early_stopping={{ pre_train.neural_network_grid.early_stopping }},
                     validation_fraction={{ pre_train.neural_network_grid.validation_fraction }},
                     beta_1={{ pre_train.neural_network_grid.beta_1 }},
                     beta_2={{ pre_train.neural_network_grid.beta_2 }},
                     epsilon={{ pre_train.neural_network_grid.epsilon }},
                     learning_rate='{{ pre_train.neural_network_grid.learning_rate }}',
                     power_t={{ pre_train.neural_network_grid.power_t }},
                     momentum={{ pre_train.neural_network_grid.momentum }},
                     nesterovs_momentum={{ pre_train.neural_network_grid.nesterovs_momentum }},
                     shuffle={{ pre_train.neural_network_grid.shuffle }},
                     learning_rate_init={{ pre_train.neural_network_grid.learning_rate_init }})

{% elif algorithm == "DECISION_TREE_REGRESSION" %}
from sklearn.tree import DecisionTreeRegressor
clf =  DecisionTreeRegressor(
                splitter = '{{ post_train.dt.splitter }}',
                max_depth = {{ post_train.dt.max_depth }},
                min_samples_leaf = {{ post_train.dt.min_samples_leaf }}
            )
{% elif algorithm == "RIDGE_REGRESSION" %}
from sklearn.linear_model import RidgeCV
clf = RidgeCV(fit_intercept=True, normalize=True)

{% elif algorithm == "LASSO_REGRESSION" %}
from sklearn.linear_model import LassoLarsIC
clf = LassoLarsIC(fit_intercept=True, normalize=True, copy_X=True)

{% elif algorithm == "LEASTSQUARE_REGRESSION" %}
from sklearn.linear_model import LinearRegression
clf = LinearRegression(fit_intercept=True, normalize=True)

{% elif algorithm == "SCIKIT_MODEL" %}
{{pre_train.custom_python.code}}

{% elif algorithm == "LARS" %}
from dataiku.doctor.prediction.lars import DkuLassoLarsRegressor
clf = DkuLassoLarsRegressor(max_var={{pre_train.lars_grid.max_features}})

{% elif algorithm == "SVM_REGRESSION" %}
from sklearn.svm import SVR
clf = SVR(
            kernel='{{ post_train.svm.kernel }}',
            C={{ post_train.svm.C }},
            {%-if post_train.svm.kernel != "linear"%}
            gamma={{ "'%s'" % post_train.svm.gamma if post_train.svm.gamma != "custom" else post_train.svm.custom_gamma }},{%endif%}
            tol={{ post_train.svm.tol }},
            max_iter={{ post_train.svm.max_iter }}
        )

{% elif algorithm == "SGD_REGRESSION" %}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(
            loss='{{ post_train.sgd.loss }}',
            penalty='{{ post_train.sgd.penalty }}',
            alpha={{ post_train.sgd.alpha }},
            {%if post_train.sgd.penalty=='elasticnet'%}
            l1_ratio={{ post_train.sgd.l1_ratio }}
            {% endif %})
{% else %}
# Algorithm unsupported: {{ algorithm }}!
{% endif %}

{% endblock %}


## The model is now being trained, we can apply it to our test set:
{% block prediction %}
%time _predictions = clf.predict(X_test)
predictions = pd.Series(data=_predictions, index=X_test.index, name='predicted_value')

# Build scored dataset
results_test = X_test.join(predictions, how='left')
results_test = results_test.join(test['__target__'], how='left')
results_test = results_test.rename(columns= {'__target__': '{{ target }}'})
{% endblock %}


{% block evaluation %}

## You can measure the model's accuracy:

{% if weight_method == "SAMPLE_WEIGHT" %}
from dataiku.doctor.prediction.regression_scoring import pearson_correlation
y_true = results_test['{{ target }}']
y_pred = results_test['predicted_value']
correlation = pearson_correlation(y_true, y_pred, sample_weight=test_sample_weight)
print ('Pearson correlation: %s' % correlation)

{% else %}
c =  results_test[['predicted_value', '{{ target }}']].corr()
print ('Pearson correlation: %s' % c['predicted_value'][1])
{% endif %}

{% endblock %}
