<div puppeteer-hook-2>
    <h1>Metrics definitions</h1>
    <dl class="metrics-definition">
        <dt>Precision</dt>:
        <dd>Proportion of correct (“positive“) predictions among “positive” predictions.</dd><br />
        <dt>Recall</dt>:
        <dd>Proportion of (correct) “positive“ predictions among “positive” records.</dd><br />
        <dt>F1-score</dt>:
        <dd>Harmonic mean between precision and recall.</dd><br />
        <dt>Accuracy</dt>:
        <dd>Proportion of correct predictions among all records ("positive" and "negative"). It is less informative than F1-score for unbalanced datasets.</dd><br />
    </dl>
</div>
<div puppeteer-hook-3>
    <h1>Cost matrix gain</h1>
    <p> You can also evaluate the average gain per record that the test set would yield by specifying a
        gain for each outcome, e.g. you win $1 for each correct prediction of
        <code ng-bind="modelData.classes[1]" /> but $-0.4 (i.e. you lose $0.4) if that prediction turns
        out to be incorrect.</p>
</div>

