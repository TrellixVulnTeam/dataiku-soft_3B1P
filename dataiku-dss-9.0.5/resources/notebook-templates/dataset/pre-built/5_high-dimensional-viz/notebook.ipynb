{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High dimensional visualization on __INPUT_DATASET_SMART_NAME__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing high dimensional data is a difficult problem and this notebook will help you apply t-SNE on your data. Although t-SNE is mathematically more complicated than other visualizations, it is also more powerful.\n",
    "\n",
    "The general idea for projecting high dimensional data into 2 dimensions is to preserve the original relative distances between points, but in the plane.\n",
    "\n",
    "A simple method called multidimensional scaling does the equivalent of putting springs between each of the points, whose strength is inversely proportional to the original distance. This will pull points together in the plane, and recover some of the original topology. However this method is not very efficient, and often falls into a local minimum that does not accurately represent the original topology.\n",
    "\n",
    "Another simple method is Sammon's mapping, in which we try harder to preserve the distances between nearby points than between those which are far apart. If two points are twice as close in the original space as two others, it is twice as important to maintain the distance between them. This method is better at revealing local, small-scale ordering.\n",
    "\n",
    "However, t-SNE does a very good job at revealing both the small scale and global topology of your high-dimensional data, and this is why we favor it. However, be aware that it can be harder to use correctly, so make sure to read the following instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of theory\n",
    "\n",
    "The t-SNE algorithm comprises two main stages.\n",
    "First, t-SNE constructs a probability distribution over pairs of your high-dimensional points, so that similar ones have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked.\n",
    "\n",
    "Second, t-SNE defines another probability distribution over the points in the two-dimensional plane, and it minimizes the Kullback–Leibler divergence (a measure of the difference between two probability distributions) between the high-dimensional and two-dimensional distributions, with respect to the locations of the points in the map.\n",
    "\n",
    "More details at https://lvdmaaten.github.io/tsne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation\n",
    "\n",
    "Before applying t-SNE, it is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high.\n",
    "\n",
    "More details at http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "In this notebook, we will display t-SNE results for both the raw data, the rescaled data and data on which PCA was applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* [Setup and loading the data](#setup)\n",
    "* [Preprocessing of the data](#preprocessing)\n",
    "* [t-Distributed Stochastic Neighbor Embedding (aka t-SNE)](#tsne)\n",
    "\n",
    "<center><strong>Select Cell > Run All to execute the whole analysis</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" /> \n",
    "\n",
    "First of all, let's load the libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import dataiku                               # Access to Dataiku datasets\n",
    "import pandas as pd, numpy as np             # Data manipulation\n",
    "from sklearn.decomposition import PCA        # PCA capability\n",
    "from sklearn.manifold import TSNE            # Import TSNE capability from scikit\n",
    "from matplotlib import pyplot as plt         # Graphing\n",
    "from collections import defaultdict, Counter # Utils\n",
    "import warnings                              # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
    "\n",
    "* Numerics\n",
    "* Categorical\n",
    "* Dates\n",
    "\n",
    "Since analyzing PCA requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample.\n",
    "\n",
    "By default, date features are not kept. Modify the following cell to change that.\n",
    "\n",
    "Also, by default, categorical columns are not included in the t-SNE data (preventing the creation of very sparse data). Modify the following cell to enable dummification.\n",
    "We also use a categorical column for coloring if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change sample size here\n",
    "dataset_limit = 10000\n",
    "# Toggle to keep dates in your dataset\n",
    "keep_dates = False\n",
    "# Toggle to dummify categorical features\n",
    "dummify_categories = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a DSS dataset as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a handle on the dataset\n",
    "mydataset = dataiku.Dataset(\"__INPUT_DATASET_SMART_NAME__\")\n",
    "\n",
    "# Load the first lines.\n",
    "# You can also load random samples, limit yourself to some columns, or only load\n",
    "# data matching some filters.\n",
    "#\n",
    "# Please refer to the Dataiku Python API documentation for more information\n",
    "df = mydataset.get_dataframe(limit = dataset_limit)\n",
    "\n",
    "df_orig = df.copy()\n",
    "\n",
    "# Get the column names\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "date_columns = list(df.select_dtypes(include=['<M8[ns]']).columns)\n",
    "\n",
    "# Print a quick summary of what we just loaded\n",
    "print \"Loaded dataset\"\n",
    "print \"   Rows: %s\" % df.shape[0]\n",
    "print \"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
    "                                                    len(numerical_columns), len(categorical_columns),\n",
    "                                                    len(date_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data <a id=\"preprocessing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle dates\n",
    "\n",
    "Keep the dates as features if requested by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "\n",
    "if keep_dates:\n",
    "    df[date_columns] = df[date_columns].astype(int)*1e-9\n",
    "else:\n",
    "    columns_to_drop.extend(date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical coloring\n",
    "You can choose which color to use here, or we will select one automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def find_color_column(df, categorical_columns):\n",
    "    color_column = []\n",
    "    if len(categorical_columns) == 0:\n",
    "        return color_column\n",
    "\n",
    "    else:\n",
    "        color_column = categorical_columns[0]\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            if df[col].nunique() > 3 and df[col].nunique() < 10:\n",
    "                color_column = col\n",
    "                break;\n",
    "    return color_column\n",
    "\n",
    "def generate_color_replacements(color_column):\n",
    "    colors = cm.rainbow(np.linspace(0, 1, df[color_column].nunique()))\n",
    "    return {key: value for (key, value) in  zip(df[color_column].unique().tolist(), colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color_column = find_color_column(df, categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value here to use a custom column for coloring your t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#color_column = 'my_color_column'\n",
    "print \"We will use column '{}' for coloring.\".format(color_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the list of colors and the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches = []\n",
    "\n",
    "if color_column:\n",
    "    cat_replacements = generate_color_replacements(color_column)    \n",
    "    coloring_list = df[color_column].map(cat_replacements)\n",
    "\n",
    "    for couple in cat_replacements:\n",
    "        patch = mpatches.Patch(color=cat_replacements[couple], label=couple.decode('utf8'))\n",
    "        patches.append(patch)\n",
    "else:\n",
    "    coloring_list = np.zeros(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional preparation\n",
    "\n",
    "Get rid of the columns that contain too many unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DROP_LIMIT_ABS = 200\n",
    "CAT_DROP_LIMIT_RATIO = 0.5\n",
    "for feature in categorical_columns:\n",
    "    nu = df[feature].nunique()\n",
    "    \n",
    "    if nu > DROP_LIMIT_ABS or nu > CAT_DROP_LIMIT_RATIO*df.shape[0]:\n",
    "        print \"Dropping feature %s with %s values\" % (feature, nu)\n",
    "        columns_to_drop.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use mean for numerical features\n",
    "for feature in numerical_columns:\n",
    "    v = df[feature].mean()\n",
    "    if np.isnan(v):\n",
    "        v = 0\n",
    "    print \"Filling %s with %s\" % (feature, v)\n",
    "    df[feature] = df[feature].fillna(v)\n",
    "    \n",
    "# Use mode for categorical features\n",
    "for feature in categorical_columns:\n",
    "    v = df[feature].value_counts().index[0]\n",
    "    df[feature] = df[feature].fillna(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Dropping the following columns: %s\" % columns_to_drop\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy encoding\n",
    "For all categorical features, we are going to \"dummy-encode\" them (also sometimes called one-hot encoding).\n",
    "\n",
    "Basically, a categorical feature is replaced by one column per value. Each created value contains 0 or 1 depending on whether the original value was the one of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For categorical variables with more than that many values, we only keep the most frequent ones\n",
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in features:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(df, [x for x in categorical_columns if not x in columns_to_drop])\n",
    "\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            dummy_name = u'%s_value_%s' % (feature, dummy_value.decode('utf-8'))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print 'Dummy-encoded feature %s' % feature\n",
    "\n",
    "dummy_encode_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling\n",
    "We use standard rescaling for applying t-SNE on the rescaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.values\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(X)\n",
    "X_std = ss.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA application\n",
    "Using principal component analysis is optional, but will help in the case of very high-dimensional data.\n",
    "\n",
    "You can set the variance to keep (90% by default) so that you end up with less dimensions, which should help the t-SNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn_pca = PCA()\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VARIANCE_TO_KEEP = 0.9\n",
    "keep_recommend = [sklearn_pca.explained_variance_ratio_[:y].sum()>VARIANCE_TO_KEEP for y in range(1,sklearn_pca.n_components_+1)].count(False)\n",
    "print \"Number of components to keep to retain %s%% of the variance:\" % (100*VARIANCE_TO_KEEP), keep_recommend, \"out of the original\", sklearn_pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sklearn_pca_final = PCA(n_components=keep_recommend)\n",
    "Y_sklearn_final = sklearn_pca_final.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Neighbor Embedding (aka t-SNE) <a id=\"tsne\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create data from 1000 first rows of the rescaled data, with PCA applied or not\n",
    "# You can change the number of rows to use here\n",
    "n=1000\n",
    "X_df = X[:n]\n",
    "X_rescaled = X_std[:n]\n",
    "X_PCA = Y_sklearn_final[:n]\n",
    "coloring_list = coloring_list[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize the t-SNE projection of the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_data = tsne.fit_transform(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,9))\n",
    "plt.scatter(tsne_data[:,0],tsne_data[:,1], c=coloring_list)\n",
    "plt.legend(handles=patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize the t-SNE projection of the rescaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_data_rescaled = tsne.fit_transform(X_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,9))\n",
    "plt.scatter(tsne_data_rescaled[:,0],tsne_data_rescaled[:,1], c=coloring_list)\n",
    "plt.legend(handles=patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize the t-SNE projection of the PCA projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_data_PCA = tsne.fit_transform(X_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,9))\n",
    "plt.scatter(tsne_data_PCA[:,0],tsne_data_PCA[:,1], c=coloring_list)\n",
    "plt.legend(handles=patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced t-SNE usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using t-SNE as an exploratory technique, it is important to understand the most common pitfalls. You can read [Wattenberg, et al. “How to Use t-SNE Effectively”, Distill, 2016](http://distill.pub/2016/misread-tsne/) for more information.\n",
    "\n",
    "We will use a list of different perplexity values to help you see if structure in your data is consistent. You can try with the raw data (X_df), rescaled data (X_std) or PCA transformed data (X_PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perplexity_list = [5, 10, 30, 50, 100]\n",
    "tsne_perp = [ TSNE(n_components=2, random_state=0, perplexity=perp) for perp in perplexity_list]\n",
    "tsne_perp_data = [ tsne.fit_transform(X_df) for tsne in tsne_perp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(len(perplexity_list)):\n",
    "    plt.subplot(3,2, i+1)\n",
    "    plt.title(\"Perplexity \" + str(perplexity_list[i]))\n",
    "    plt.scatter(tsne_perp_data[i][:,0],tsne_perp_data[i][:,1], c=coloring_list)\n",
    "plt.subplot(3,2, i+2)\n",
    "plt.axis('off')\n",
    "plt.legend(handles=patches, loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
